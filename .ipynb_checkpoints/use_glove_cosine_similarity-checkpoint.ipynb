{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "**Google's Universal Sentence Encoder (USE)** provides 512-dimension vectors for each input that are pre-trained on large corpus, and can be plugged into a variety of different task models, such as sentiment analysis, classification, and etc. It is speed-efficient without losing task accuracy, and also provides embeddings not just for word level, but also for phrases, sentences, and even paragraphs. However, the more the words are given as input, the more likely each word meaning gets diluted.\n",
    "\n",
    "This notebook is based on the Semantic Similarity with TF-Hub Universal Encoder tutorial, but uses a separate input from one of the projects. We will also use **GloVe** vectors to compare how the vectors and cosine similarity differ between the two models. \n",
    "\n",
    "- First, the notebook goes over setting up locally and use one sample data to create embeddings saved out as a separate csv file using Pandas.\n",
    "\n",
    "- Then assuming you have cloned the repository, we call in custom functions to quickly extract vectors of given word, phrase, sentences in USE and GloVe. \n",
    "\n",
    "### Table of Contents/Short-cuts: \n",
    "1. [About USE Models and Deep Average Network](#About-USE-Models-and-Deep-Average-Network)\n",
    "2. [Installation & Setup](#Installation-&-Setup)\n",
    "3. [Path Setup](#Path-Setup)\n",
    "4. [Raw Data Format](#Raw-Data-Format)\n",
    "5. [Get USE Embeddings](#Get-USE-Embeddings)\n",
    "6. [Cosine Similarity](#Cosine-Similarity)\n",
    "7. [Cosine Similarity Examples](#Cosine-Similarity-Example)\n",
    "8. [Plotting Similarity Matrix](#Plotting-Similarity-Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About USE Models and Deep Average Network\n",
    "There are two types of models in **USE**: **Transformer** and **Deep Averaging Network (DAN)**. We will use DAN which is a lighter version for efficiency and speed in exchange for reduced accuracy (still accurate enough). \n",
    "\n",
    "DAN first averages the input word embeddings to create a sentence embedding. It uses PTB tokenizer, which divides a sentence into a sequence of tokens based on set of rules on  how to process punctuation, articles, etc, in order to create 512 dimension embeddings. This averaged 512 vector is passed to one or more feedforward layers. Then it is multi-task-trained on unsupervised data drawn from various internet sources,  Wikipedia, Stanford Natural Language Inference corpus, web news, and forums. \n",
    "- Training  goals: \n",
    "    - Uses skip-thought-like model that predicts the surrounding sentences of a given text (see below)\n",
    "    - Conversational response suggestion \n",
    "    - Classification task on supervised data\n",
    "\n",
    "The intuition behind deep feedforward neural network is that each layer learns a more abstract representation of the input than the previous one. So its depth allows to capture subtle variations of the input with more depths. Also, each layer only involves a single matrix multiplication, allowing minimal computing time.\n",
    "\n",
    "See full USE paper: https://arxiv.org/pdf/1803.11175.pdf\n",
    "See full DAN paper: https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation & Setup\n",
    "I used Anaconda to create a TensorFlow-specific environment to customize the package versions. After installing Anaconda...\n",
    "\n",
    "1. Creating a new environment:\n",
    "```\n",
    "conda create -n py3 python=3.6.8\n",
    "```\n",
    "2. Activate the created environment by `conda activate py3`\n",
    "\n",
    "3. Using pip, install packages for pandas, numpy, seaborn, tensorflow, tensorflow_hub. ie. `pip install pckge-name`\n",
    "\n",
    "4. Then, let's make sure to set the packages to exact version:\n",
    "```\n",
    "pip install --upgrade tensorflow=1.15.0\n",
    "pip install --upgrade tensorflow-hub=0.7.0\n",
    "```\n",
    "\n",
    "Once the steps are done, we should be able to run the codes locally.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "due to some depecrated methods and changes made with the tf version upgrade from tf1.X to tf2.0, here we use a specific set of Python and tf versions. You can check via `pip freeze`\n",
    "\n",
    "- tested on python == 3.6.8 | tensorflow == 1.15.0 | tensorflow_hub == 0.7.0\n",
    "\n",
    "Or you can check the version in Python via:\n",
    "```python\n",
    "import sys\n",
    "print(sys.version_info)  # sys.version_info(major=3, minor=6, micro=8, releaselevel='final')\n",
    "print(tf.__version__)    # '1.15.0'\n",
    "print(hub.__version__)   # '0.7.0'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script variables\n",
    "\n",
    "# for lite/DAN version:\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\"\n",
    "\n",
    "# for heavy/Transformer version:\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "\n",
    "baseDir = 'use-glove-narrative'  # repository/base folder name\n",
    "embedding_size = 512  # base 512-dimension embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Path Setup\n",
    "Assuming that you git cloned the project (which is for demo purposes) to your local directory, we set the path so the code knows where to look for certain data files using the `baseDir` specified above. We will mainly just work within the cloned folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "# recursively find absolute path\n",
    "while os.path.basename(pwd) != baseDir:\n",
    "    os.chdir('..')\n",
    "    pwd = os.getcwd()\n",
    "baseDir = pwd\n",
    "dataDir = baseDir + '/data'\n",
    "\n",
    "# recursively find all csv files. We will work with one file here\n",
    "all_csvs = [y for x in os.walk(dataDir) for y in glob(os.path.join(x[0], '*.csv'))]\n",
    "all_csvs.sort()\n",
    "all_csvs = all_csvs[0]  # we will just use one sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Format\n",
    "To briefly show the data, the data is comprised of numerous idea units, or phrases of words with unique meanings. Here, we are only interested in the 'text' column and 'index' column. We will call in the text of the entire story to create embeddings for each idea unit. Below the example print out, we will loop over each story to create embeddings. Since we will use one story this time, it shouldn't take that long. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>scoring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>One fine day an old Maine man was fishing</td>\n",
       "      <td>mentions at least 3 of the following: “old”, “...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>on his favorite lake</td>\n",
       "      <td>mentions “favorite lake” or “favorite river”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>and catching very little.</td>\n",
       "      <td>mentions that he/the fisherman was not having ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, he gave up</td>\n",
       "      <td>mentions that he gave up/stopped fishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>and walked back along the shore to his fishing...</td>\n",
       "      <td>mentions that he walked home/to his fishing sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paragraph  index                                               text  \\\n",
       "0          1      1         One fine day an old Maine man was fishing    \n",
       "1          1      2                              on his favorite lake    \n",
       "2          1      3                          and catching very little.   \n",
       "3          1      4                                Finally, he gave up   \n",
       "4          1      5  and walked back along the shore to his fishing...   \n",
       "\n",
       "                                             scoring  \n",
       "0  mentions at least 3 of the following: “old”, “...  \n",
       "1       mentions “favorite lake” or “favorite river”  \n",
       "2  mentions that he/the fisherman was not having ...  \n",
       "3           mentions that he gave up/stopped fishing  \n",
       "4  mentions that he walked home/to his fishing sh...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example print of data format\n",
    "datafile = pd.read_csv(all_csvs)\n",
    "datafile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Get USE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read in the data file\n",
    "textfile = pd.read_csv(all_csvs)\n",
    "# get the title of the narrative story, cutting out the .csv extension\n",
    "title = os.path.basename(all_csvs)[:-4]\n",
    "\n",
    "\n",
    "# create df to save out at the end\n",
    "vector_df_columns = ['paragraph', 'index', 'text', 'size']\n",
    "# create column for each dimension (out of 512)\n",
    "for i in range(1, embedding_size + 1):\n",
    "    vector_df_columns.append('dim' + str(i))\n",
    "vector_df = pd.DataFrame(columns=vector_df_columns)\n",
    "\n",
    "\n",
    "# import the Universal Sentence Encoder's TF Hub module\n",
    "embed = hub.Module(module_url)  # hub.load(module_url) for tf==2.0.0\n",
    "\n",
    "# we call in the text column from data file\n",
    "messages = []\n",
    "for t in range(0, len(textfile)):\n",
    "    messages.append(textfile.iloc[t]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>size</th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>...</th>\n",
       "      <th>dim503</th>\n",
       "      <th>dim504</th>\n",
       "      <th>dim505</th>\n",
       "      <th>dim506</th>\n",
       "      <th>dim507</th>\n",
       "      <th>dim508</th>\n",
       "      <th>dim509</th>\n",
       "      <th>dim510</th>\n",
       "      <th>dim511</th>\n",
       "      <th>dim512</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>One fine day an old Maine man was fishing</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0169429</td>\n",
       "      <td>-0.0030699</td>\n",
       "      <td>-0.0156278</td>\n",
       "      <td>-0.00649163</td>\n",
       "      <td>0.0213989</td>\n",
       "      <td>-0.0541645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0404136</td>\n",
       "      <td>-0.0577177</td>\n",
       "      <td>0.0108959</td>\n",
       "      <td>-0.0337963</td>\n",
       "      <td>0.0817816</td>\n",
       "      <td>-0.074783</td>\n",
       "      <td>0.0231454</td>\n",
       "      <td>0.0719041</td>\n",
       "      <td>-0.047105</td>\n",
       "      <td>0.0127639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>on his favorite lake</td>\n",
       "      <td>512</td>\n",
       "      <td>-0.0172151</td>\n",
       "      <td>0.0418602</td>\n",
       "      <td>0.0105562</td>\n",
       "      <td>0.0290091</td>\n",
       "      <td>0.0351211</td>\n",
       "      <td>-0.0121579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0319399</td>\n",
       "      <td>-0.0201722</td>\n",
       "      <td>-0.00480706</td>\n",
       "      <td>-0.0490393</td>\n",
       "      <td>0.0562807</td>\n",
       "      <td>-0.0840528</td>\n",
       "      <td>0.0359073</td>\n",
       "      <td>0.0519214</td>\n",
       "      <td>0.0635523</td>\n",
       "      <td>-0.0615548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>and catching very little.</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0515628</td>\n",
       "      <td>0.00556853</td>\n",
       "      <td>-0.0606079</td>\n",
       "      <td>-0.0281095</td>\n",
       "      <td>-0.0631535</td>\n",
       "      <td>-0.0586548</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0266129</td>\n",
       "      <td>0.0111167</td>\n",
       "      <td>-0.0238963</td>\n",
       "      <td>0.00741908</td>\n",
       "      <td>-0.0685881</td>\n",
       "      <td>-0.0858848</td>\n",
       "      <td>0.066858</td>\n",
       "      <td>-0.0616563</td>\n",
       "      <td>-0.0844253</td>\n",
       "      <td>-0.026685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, he gave up</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0818241</td>\n",
       "      <td>0.00549721</td>\n",
       "      <td>-0.0245033</td>\n",
       "      <td>0.0286504</td>\n",
       "      <td>-0.0284165</td>\n",
       "      <td>-0.0575481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0474779</td>\n",
       "      <td>-0.00603216</td>\n",
       "      <td>-0.0116888</td>\n",
       "      <td>-0.06419</td>\n",
       "      <td>0.0268704</td>\n",
       "      <td>-0.0640136</td>\n",
       "      <td>0.103409</td>\n",
       "      <td>-0.0235997</td>\n",
       "      <td>-0.0781731</td>\n",
       "      <td>-0.0365196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>and walked back along the shore to his fishing...</td>\n",
       "      <td>512</td>\n",
       "      <td>0.00286058</td>\n",
       "      <td>0.0576001</td>\n",
       "      <td>0.0103945</td>\n",
       "      <td>-0.00301533</td>\n",
       "      <td>0.0199591</td>\n",
       "      <td>-0.0644398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0145959</td>\n",
       "      <td>0.0137776</td>\n",
       "      <td>0.0165417</td>\n",
       "      <td>-0.0406641</td>\n",
       "      <td>-0.0204453</td>\n",
       "      <td>-0.0713526</td>\n",
       "      <td>0.0121754</td>\n",
       "      <td>0.00591647</td>\n",
       "      <td>0.0262764</td>\n",
       "      <td>-0.0329477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 516 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  paragraph index                                               text size  \\\n",
       "0         1     1         One fine day an old Maine man was fishing   512   \n",
       "1         1     2                              on his favorite lake   512   \n",
       "2         1     3                          and catching very little.  512   \n",
       "3         1     4                                Finally, he gave up  512   \n",
       "4         1     5  and walked back along the shore to his fishing...  512   \n",
       "\n",
       "         dim1        dim2       dim3        dim4       dim5       dim6  ...  \\\n",
       "0   0.0169429  -0.0030699 -0.0156278 -0.00649163  0.0213989 -0.0541645  ...   \n",
       "1  -0.0172151   0.0418602  0.0105562   0.0290091  0.0351211 -0.0121579  ...   \n",
       "2   0.0515628  0.00556853 -0.0606079  -0.0281095 -0.0631535 -0.0586548  ...   \n",
       "3   0.0818241  0.00549721 -0.0245033   0.0286504 -0.0284165 -0.0575481  ...   \n",
       "4  0.00286058   0.0576001  0.0103945 -0.00301533  0.0199591 -0.0644398  ...   \n",
       "\n",
       "      dim503      dim504      dim505      dim506     dim507     dim508  \\\n",
       "0  0.0404136  -0.0577177   0.0108959  -0.0337963  0.0817816  -0.074783   \n",
       "1  0.0319399  -0.0201722 -0.00480706  -0.0490393  0.0562807 -0.0840528   \n",
       "2 -0.0266129   0.0111167  -0.0238963  0.00741908 -0.0685881 -0.0858848   \n",
       "3  0.0474779 -0.00603216  -0.0116888    -0.06419  0.0268704 -0.0640136   \n",
       "4 -0.0145959   0.0137776   0.0165417  -0.0406641 -0.0204453 -0.0713526   \n",
       "\n",
       "      dim509      dim510     dim511     dim512  \n",
       "0  0.0231454   0.0719041  -0.047105  0.0127639  \n",
       "1  0.0359073   0.0519214  0.0635523 -0.0615548  \n",
       "2   0.066858  -0.0616563 -0.0844253  -0.026685  \n",
       "3   0.103409  -0.0235997 -0.0781731 -0.0365196  \n",
       "4  0.0121754  0.00591647  0.0262764 -0.0329477  \n",
       "\n",
       "[5 rows x 516 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce logging output.\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "with tf.compat.v1.Session() as session:\n",
    "    session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n",
    "    message_embeddings = session.run(embed(messages))\n",
    "\n",
    "# make sure all units are there/sanity check\n",
    "assert len(message_embeddings) == len(textfile) == len(messages)\n",
    "\n",
    "# loop over each vector value to corresponding text\n",
    "for e in range(0, len(message_embeddings)):\n",
    "    vector_df.at[e, 'paragraph'] = textfile.iloc[e]['paragraph']\n",
    "    vector_df.at[e, 'index'] = textfile.iloc[e]['index']\n",
    "    vector_df.at[e, 'text'] = messages[e]\n",
    "    vector_df.at[e, 'size'] = len(message_embeddings[e])\n",
    "    for dim in range(0, len(message_embeddings[e])):\n",
    "        vector_df.at[e, 'dim'+str(dim+1)] = message_embeddings[e][dim]\n",
    "\n",
    "# display sample format\n",
    "vector_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 516)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(vector_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample data shows each idea unit/text converted to 512 dimension vectors. `np.shape(vector_df)` will return a 41 total idea units/phrases to 516 columns (512 dimensions + custom columns (paragraph info, index, text, and size)). We then use these vectors to explore semantic similarity between text and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the code below to save out as csv file\n",
    "vector_df.reindex(columns=vector_df_columns)\n",
    "vector_df.to_csv(title + '_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## Cosine Similarity\n",
    "As a brief description, **cosine similarity** is basically the measure of cosine angle between the two vectors. Since we have USE and GloVe vectors that represent words into multidimensional vectors, we can apply these vector values to calculate how similar the two words are. \n",
    "\n",
    "It can be easily calculated in Python with its useful packages:\n",
    "```python\n",
    "cos_sim = numpy.dot(vector1, vector2)/(numpy.linalg.norm(vector1) * numpy.linalg.norm(vector2))\n",
    "```\n",
    "\n",
    "Assuming we established some basic understanding, let's call in the functions I made so that we can easily get USE and GloVe vectors at multiple word level. \n",
    "\n",
    "I will highlight some of the functions below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function glove_vec in module get_glove_use:\n",
      "\n",
      "glove_vec(item1, item2)\n",
      "    get vectors for given two words and calculate cosine similarity\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    item1 : str\n",
      "        string in glove word pool vector to compare\n",
      "    item2 : str\n",
      "        string in glove word pool vector to compare\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    item1_vector : array\n",
      "        item1 GloVe vector\n",
      "    item2_vector : array\n",
      "        item2 GloVe vector\n",
      "    cos_sim : float\n",
      "        cosine similarity of item1 and item2 vectors\n",
      "\n",
      "Help on function use_vec in module get_glove_use:\n",
      "\n",
      "use_vec(item1, item2)\n",
      "    get USE vectors and cosine similairty of the two items\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    item1 : str, list\n",
      "        any word to compare, put in string for more than one word\n",
      "    item2 : str, list\n",
      "        any word to compare, put in string for more than one word\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    item1_vector : array\n",
      "        item1 USE vector\n",
      "    item2_vector : array\n",
      "        item2 USE vector\n",
      "    cos_sim : float\n",
      "        cosine similarity of item1 and item2 vectors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from get_glove_use import *\n",
    "help(glove_vec)\n",
    "help(use_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Example\n",
    "Using the two functions above, and another function compare_word_vec (which basically uses the two functions), we can easily obtain cosine similarity of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11964830574261577\n",
      "0.5305143\n"
     ]
    }
   ],
   "source": [
    "# using the two functions above, we can get \n",
    "# GloVe and USE vectors and cosine similarity of two input words\n",
    "os.chdir(gloveDir)\n",
    "_, _, glove_sim = glove_vec('fish','bear')\n",
    "_, _, use_sim = use_vec('fish','bear')\n",
    "print('use cos: ' + str(use_sim))\n",
    "print('glove cos: ' + str(glove_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use cos: 0.49838725\n",
      "glove cos: 0.18601566881803455\n"
     ]
    }
   ],
   "source": [
    "# the two functions glove_vex and use_vec are use in compare_word_vec\n",
    "compare_word_vec('man','fish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** From the example above, USE and GloVe similarly identy _fish_ to be somewhat equally similar to _bear_ and _man_ (but just in different scale/degree). \n",
    "\n",
    "Now let's try comparing at multiple words or phrase level. We will use new functions and give in new inputs as strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old man caught fish & bear hunted trout:\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove sim: 0.36609688461789297\n",
      "USE sim: 0.50494814\n",
      "old man caught fish & bear eat six fish:\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove sim: 0.6818474640845398\n",
      "USE sim: 0.5896743\n",
      "bear hunted trout & bear eat six fish:\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove sim: 0.6082457470353315\n",
      "USE sim: 0.72352856\n"
     ]
    }
   ],
   "source": [
    "sentence1 = ['old', 'man', 'caught', 'fish']\n",
    "sentence2 = ['bear', 'hunted', 'trout']\n",
    "sentence3 = ['bear','eat','six','fish']\n",
    "\n",
    "print('old man caught fish & bear hunted trout:')\n",
    "phrase_vec(sentence1, sentence2)\n",
    "\n",
    "print('old man caught fish & bear eat six fish:')\n",
    "phrase_vec(sentence1, sentence3)\n",
    "\n",
    "print('bear hunted trout & bear eat six fish:')\n",
    "phrase_vec(sentence2, sentence3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** From the example above, we can see that USE and GloVe capture somewhat differently. We can see that _bear hunted trout_ and _bear eat six fish_ are the most similar to each other, whereas _old man caught fish_ is also similar to the context of bear eating six fish. \n",
    "\n",
    "More detailed analysis is required, but the example above shows great possibilities to exploring semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Similarity Matrix\n",
    "Now that we can compare similarity of words and sentences, we can plot a simple pairwise matrix, which basically compares how similar each word is to another in the given list. Fortunately, we already have a plot for doing it (using Seaborn). \n",
    "\n",
    "I will only use few words as demonstration, since it's been slowing up my computer so much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sim_matrix(['man', 'bear', 'fish', 'trout'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ending Note\n",
    "\n",
    "In the example above, we only used simple noun words. The stronger blue color, the more similar the two words are. Thus, the diagonal strip is deep blue (similarity of same two words is 1). You can see _fish_ and _trout_ are more similar to each other, than is _man_ to _trout_.\n",
    "\n",
    "Keep in mind that you can feed in more words and sentences to create and visualize a larger matrix. \n",
    "\n",
    "We looked at setting up USE locally, and creating embeddings from USE. The cloned project also has sample version of GloVe vectors. We use the vectors from the two models to extract vectors and compare similarity of two texts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
